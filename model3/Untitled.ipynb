{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f3fb4c2-6cc7-48f1-95da-16865ee4db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae5c1c7f-32ca-4db4-a20f-c24764eb1fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(string: str, \n",
    "               punctuations = r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "               stop_words = stopwords.words('english'),\n",
    "               # porter = PorterStemmer()\n",
    "               wnl = WordNetLemmatizer()\n",
    "              ):\n",
    "    \"\"\"\n",
    "    A method to clean text. It removes punctuations, stop words, applies lemmatization.\n",
    "    \"\"\"\n",
    "    # Removing the punctuations\n",
    "    for x in string.lower(): \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\") \n",
    "\n",
    "    # Converting the text to lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Removing stop words\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    # stemming/lemmatizing words. That means changing word to its basic format, for example\n",
    "    # words 'fishing', 'fished', 'fischer' will be changed into a word 'fisch'\n",
    "    # lemmatization should be better because stemming changes words too much, for example\n",
    "    # business is changed into busi\n",
    "    # string = ' '.join([porter.stem(word) for word in string.split()])\n",
    "    string = ' '.join([wnl.lemmatize(word, pos = \"v\") for word in string.split()])\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "    return string\n",
    "\n",
    "def create_training_data(tokenizer,\n",
    "                         max_sen_len,\n",
    "                         sentences_file,\n",
    "                         embed_matrix_file,\n",
    "                         model_folder\n",
    "                        ):\n",
    "    \"\"\"\n",
    "    Creating a training and testing datasets self.x_train, self.x_test, self.y_train, self.y_test. This function\n",
    "    also creates and saves a tokenizer and a list of all unique tables names all_unique_values because when we load\n",
    "    a ready model those values are needed for the 'predict' function.\n",
    "    \"\"\"\n",
    "    sentences_tables = pd.read_excel(sentences_file).values\n",
    "    random.shuffle(sentences_tables)\n",
    "    clean_sentences = np.array([clean_text(sentence) for sentence in sentences_tables[:, 0]])\n",
    "\n",
    "    tokenizer.fit_on_texts(clean_sentences)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(clean_sentences)\n",
    "    x = pad_sequences(sequences, maxlen = max_sen_len)\n",
    "\n",
    "    embed_matrix = pd.read_csv(embed_matrix_file).values\n",
    "\n",
    "    x_train, x_test = train_test_split(x, test_size = 0.2)\n",
    "\n",
    "    with open(os.path.join(model_folder, 'tokenizer.json'), 'w') as file:\n",
    "        json.dump(tokenizer.to_json(), file)\n",
    "        \n",
    "    return x_train, x_test\n",
    "\n",
    "\n",
    "def get_coefs(word, *arr): \n",
    "    return word, list(np.asarray(arr, dtype='float'))\n",
    "\n",
    "\n",
    "def create_embedding_file(tokenizer,\n",
    "                          embed_file_src = r'model\\glove.840B.300d.txt', \n",
    "                          embed_file_trg = r'model\\model_embeddings.txt'\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    This function will create an embedding file called embed_file_trg which will contain only those words \n",
    "    from embed_file_src which are present in the training dataset. If training dataset wasn't created yet\n",
    "    then this function will create it.\n",
    "    \"\"\"\n",
    "    # creating a training dataset if we didn't do that yet\n",
    "    # if not hasattr(self, 'tokenizer'):\n",
    "    #     self.create_training_data()\n",
    "\n",
    "    embeddings = dict(get_coefs(*o.split(\" \")) for o in open(embed_file_src, errors = 'ignore'))\n",
    "    with open(embed_file_trg, 'w') as file:\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            word_vector = embeddings[word]\n",
    "            line = ' '.join(np.concatenate([[word], word_vector]))\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "\n",
    "def create_embedding_matrix(tokenizer,\n",
    "                            model_folder,\n",
    "                            word_vec_dim,\n",
    "                            embed_file_path,\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    A function to create the embedding matrix. This is a matrix where each row is a vector representing a word.\n",
    "    To create that matrix we use a word embedding file which path is equal to embedding_file_path.\n",
    "    embedding_matrix[row_number] is a vector representation for a word = list(tokenizer.word_index.keys())[row_number - 1]\n",
    "    First row of embedding_matrix are zeros. This matrix is needed to train a model.\n",
    "    \"\"\"\n",
    "    embeddings = dict(get_coefs(*o.split(\" \")) for o in open(embed_file_path, errors = 'ignore'))\n",
    "\n",
    "    # embedding_matrix[row_number] is a vector representation of a word = self.tokenizer.word_index.keys()[row_number - 1]\n",
    "    # first row in embedding_matrix is 0\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_counts) + 1, word_vec_dim))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index > len(tokenizer.word_counts):\n",
    "            break\n",
    "        else:\n",
    "            try:\n",
    "                embedding_matrix[index] = embeddings[word]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    pd.DataFrame(embedding_matrix).to_csv(os.path.join(model_folder, 'embedding_matrix.csv'))\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a6c030-6b64-4d53-9cdf-18ae47d5b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "max_sen_len = 20\n",
    "sentences_file = r'data\\sentences_tables.xlsx'\n",
    "embed_matrix_file = r'model\\embedding_matrix.csv'\n",
    "model_folder = 'model'\n",
    "word_vec_dim = 300\n",
    "embed_file_path = r'model\\model_embeddings.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa21b414-1a9d-4045-b551-9416b28cf16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = create_training_data(\n",
    "    tokenizer = tokenizer, \n",
    "    max_sen_len = max_sen_len,\n",
    "    sentences_file = sentences_file,\n",
    "    embed_matrix_file = embed_matrix_file,\n",
    "    model_folder = model_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "703325ad-5693-45fb-8ca5-0632a439fcfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ..., 14, 26, 16],\n",
       "       [ 0,  0,  0, ...,  0, 11, 15],\n",
       "       [ 0,  0,  0, ..., 10,  2, 27],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  1, 13, 12],\n",
       "       [ 0,  0,  0, ..., 18, 23, 13],\n",
       "       [ 0,  0,  0, ..., 22, 37, 41]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fbd939a-fe1a-4b2d-9b3b-b46858c3282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_matrix = create_embedding_matrix(\n",
    "    tokenizer = tokenizer,\n",
    "    model_folder = model_folder,\n",
    "    word_vec_dim = word_vec_dim,\n",
    "    embed_file_path = embed_file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a69efdd-4542-4dfc-8cb3-d65237cdf61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [-0.50318 ,  0.27905 , -0.045497, ...,  0.4781  ,  0.13005 ,\n",
       "        -0.014399],\n",
       "       [ 0.16082 ,  0.11008 , -0.28129 , ...,  0.12435 , -0.27433 ,\n",
       "         0.65513 ],\n",
       "       ...,\n",
       "       [ 0.37492 , -0.052425, -0.60094 , ..., -0.36104 , -0.065253,\n",
       "        -0.1206  ],\n",
       "       [ 0.012832,  0.22669 , -0.17511 , ...,  0.17134 ,  0.040047,\n",
       "        -0.37131 ],\n",
       "       [-0.39054 , -0.55117 , -0.073466, ...,  0.34569 ,  0.30918 ,\n",
       "        -0.32873 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "130632c9-bbf4-4a09-a7e1-e8c153935bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embedding_dim,\n",
    "                 units,\n",
    "                 batch_size,\n",
    "                 embed_matrix\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = layers.Embedding(\n",
    "            input_dim = embed_matrix.shape[0],\n",
    "            output_dim = embedding_dim,\n",
    "            embeddings_initializer = tf.keras.initializers.Constant(embed_matrix),\n",
    "            trainable = False\n",
    "        )\n",
    "        self.lstm = layers.LSTM(\n",
    "            units = self.units,\n",
    "            return_sequences = True,\n",
    "            return_state = True\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def call(self, x, state_h, state_c):\n",
    "        # make sure that the types are correct\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        state_h = tf.cast(state_h, tf.float32)\n",
    "        state_c = tf.cast(state_c, tf.float32)\n",
    "        # x.shape = (batch_size, max_sen_len)\n",
    "        # x is a series of numbers which represent words\n",
    "        # state_h.shape = (batch_size, state_size)\n",
    "        x = self.embedding(x)\n",
    "        # x = tf.reshape(x, [1, x.shape[0], x.shape[1]])\n",
    "        output, state_h, state_c = self.lstm(x, initial_state = [state_h, state_c])\n",
    "        return output, state_h, state_c\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        state_h = tf.zeros((self.batch_size, self.units))\n",
    "        state_c = tf.zeros((self.batch_size, self.units))\n",
    "        return state_h, state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e586409-6d87-463c-90dd-40135c565e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1 like in Jonathan Hui pdf\n",
    "class Bahdau_attention(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.W2 = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "        \n",
    "    def call(self, decoder_hidden, encoder_hidden):\n",
    "        # decoder_hidden.shape = (batch_size, hidden_size)\n",
    "        # decoder_hidden_time_axis.shape = (batch_size, 1, hidden_size)\n",
    "        decoder_hidden_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
    "        \n",
    "        # encoder_hidden.shape = (batch_size, max_sen_len, hidden_size)\n",
    "        # argument for tanh shape = (batch_size, max_sen_len, hidden_size)\n",
    "        # score.shape = (batch_size, max_sen_len, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(decoder_hidden_time_axis) + self.W2(encoder_hidden)))\n",
    "        \n",
    "        # attention_weights.shape = (batch_size, max_sen_len, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis = 1)\n",
    "        \n",
    "        # context_vector.shape = (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * encoder_hidden\n",
    "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "398014c9-b9ab-4f29-84d1-7c96af06a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2, like in 'attention explanation.pdf'\n",
    "class Bahdau_attention(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = layers.Dense(1)\n",
    "        \n",
    "    def call(self, decoder_state_h, encoder_states_h):\n",
    "        # decoder_state_h.shape = (batch_size, hidden_size)\n",
    "        # encoder_states_h.shape = (batch_size, max_sen_len, hidden_size)\n",
    "        # encoder_states_h_flattened.shape = (max_sen_len, hidden_size)\n",
    "        encoder_states_h_flattened = tf.reshape(encoder_states_h, [-1, tf.shape(encoder_states_h)[2]])\n",
    "        try:\n",
    "            batch_size, hidden_size = tf.gather(tf.shape(decoder_state_h), [0,1])\n",
    "        except:\n",
    "            print(decoder_state_h)\n",
    "        # context_vector.shape = (batch_size, hidden_size)\n",
    "        context_vector = np.zeros([batch_size, hidden_size])\n",
    "        for i in range(batch_size):\n",
    "            for j in range(max_sen_len):\n",
    "                e = []\n",
    "                for k in range(max_sen_len):\n",
    "                    x = tf.concat([decoder_state_h[i], encoder_states_h_flattened[i + k]], 0)\n",
    "                    x = tf.reshape(x, [1, tf.shape(x)[0]])\n",
    "                    e.append(tf.math.exp(self.dense(x)))\n",
    "                e = tf.stack(e)\n",
    "\n",
    "                attention_weight = e[j] / tf.math.reduce_sum(e)\n",
    "                context_vector[i] += attention_weight * encoder_states_h_flattened[i + j]\n",
    "        \n",
    "        return tf.cast(tf.stack(context_vector), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "15a6c0cc-731a-475c-bc69-bea3475e4fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units, batch_size, embed_matrix):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = layers.Embedding(\n",
    "            input_dim = embed_matrix.shape[0],\n",
    "            output_dim = embedding_dim,\n",
    "            embeddings_initializer = tf.keras.initializers.Constant(embed_matrix),\n",
    "            trainable = False\n",
    "        )\n",
    "        self.lstm = layers.LSTM(\n",
    "            units = self.units,\n",
    "            return_sequences = True,\n",
    "            return_state = True\n",
    "        )\n",
    "        self.dense = layers.Dense(vocab_size)\n",
    "        self.attention = Bahdau_attention()\n",
    "        \n",
    "    def call(self, x, decoder_state_h, decoder_state_c, encoder_states_h):\n",
    "        # x.shape = (batch_size, 1)\n",
    "        # x is a single number for each batch representing a single word\n",
    "        # encoder_states_h.shape = (batch_size, max_sen_len, hidden_size)\n",
    "        # decoder_state_h.shape = (batch_size, 1, hidden_size)\n",
    "        \n",
    "        # make sure that the types are correct\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        decoder_state_h = tf.cast(decoder_state_h, tf.float32)\n",
    "        decoder_state_c = tf.cast(decoder_state_c, tf.float32)\n",
    "        encoder_states_h = tf.cast(encoder_states_h, tf.float32)\n",
    "        \n",
    "        # context_vector.shape = (batch_size, hidden_size)\n",
    "        context_vector = self.attention(decoder_state_h, encoder_states_h)\n",
    "        # shape of output of embedding layer = (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        # x.shape after concatenation = (batch_size, 1, hidden_size + embedding_dim)\n",
    "        try:\n",
    "            x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = 2)\n",
    "        except Exception as e:\n",
    "            print(context_vector)\n",
    "            print(x)\n",
    "            print(e)\n",
    "            return\n",
    "        \n",
    "        # output, state_h, state_c = self.lstm(x)\n",
    "        # output, state_h, state_c = self.lstm(x, initial_state = [tf.reshape(decoder_state_h, [decoder_state_h.shape[0], decoder_state_h.shape[2]]),\n",
    "        #                                                          tf.reshape(decoder_state_c, [decoder_state_c.shape[0], decoder_state_c.shape[2]])\n",
    "        #                                                         ])\n",
    "        output, state_h, state_c = self.lstm(x, initial_state = [decoder_state_h, decoder_state_c])\n",
    "        \n",
    "        output = tf.reshape(output, [-1, tf.shape(output)[2]])\n",
    "        # output.shape = (batch_size, vocab_size)\n",
    "        output = self.dense(output)\n",
    "        \n",
    "        return output, state_h, state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "53ffc42e-fa8e-4ca6-956a-948ae38af438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inp, \n",
    "               targ, \n",
    "               enc_state_h, \n",
    "               enc_state_c, \n",
    "               batch_size, \n",
    "               encoder, \n",
    "               decoder, \n",
    "               loss_function, \n",
    "               optimizer):\n",
    "    # inp.shape = targ.shape (batch_size, max_sen_len, 1)\n",
    "    # enc_state_h.shape = (batch_size, state_size)\n",
    "    \n",
    "    # make sure that the types are correct\n",
    "    inp = tf.cast(inp, tf.float32)\n",
    "    targ = tf.cast(targ, tf.float32)\n",
    "    enc_state_h = tf.cast(enc_state_h, tf.float32)\n",
    "    enc_state_c = tf.cast(enc_state_c, tf.float32)\n",
    "    \n",
    "    batch_loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # enc_output.shape = (batch_size, max_sen_len, state_size)\n",
    "        # enc_state_h.shape = (batch_size, state_size)\n",
    "        enc_output, enc_state_h, enc_state_c = encoder(inp, enc_state_h, enc_state_c)\n",
    "        dec_state_h = enc_state_h\n",
    "        dec_state_c = enc_state_c\n",
    "        \n",
    "        # dec_input.shape = (batch_size, 1)\n",
    "        dec_input = tf.expand_dims([0] * batch_size, 1)\n",
    "        \n",
    "        for t in range(len(targ)):\n",
    "            prediction, dec_state_h, dec_state_c, = decoder(dec_input, dec_state_h, dec_state_c, enc_output)\n",
    "            batch_loss += loss_function(targ[:, t], prediction)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "            \n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(batch_loss, variables)\n",
    "    optimizer.apply_gradients(grads_and_vars = zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "16035f6f-2d3d-43bd-8f41-174494e45259",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype = loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "423aca49-8c11-4305-bb06-80e0091323dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sen_len = 3\n",
    "batch_size = 2\n",
    "\n",
    "inp = tf.constant([[1,2,3], [4,5,6]])\n",
    "targ = tf.constant([[1,2,3], [4,5,6]])\n",
    "enc_state_h = tf.constant([[1,2,3], [4,5,6]])\n",
    "enc_state_c = tf.constant([[1,2,3], [4,5,6]])\n",
    "\n",
    "decoder = Decoder(vocab_size = 1000,\n",
    "                  embedding_dim = 300,\n",
    "                  units = 3,\n",
    "                  batch_size = 2,\n",
    "                  embed_matrix = embed_matrix\n",
    "                 )\n",
    "\n",
    "encoder = Encoder(vocab_size = 1000,\n",
    "                  embedding_dim = 300,\n",
    "                  units = 3,\n",
    "                  batch_size = 2,\n",
    "                  embed_matrix = embed_matrix\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "08ddabb1-8f5b-4761-81bc-9273ce442ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['decoder_22/bahdau_attention_22/dense_45/kernel:0', 'decoder_22/bahdau_attention_22/dense_45/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['decoder_22/bahdau_attention_22/dense_45/kernel:0', 'decoder_22/bahdau_attention_22/dense_45/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=13.36879>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step(inp = inp, \n",
    "           targ = targ, \n",
    "           enc_state_h = enc_state_h, \n",
    "           enc_state_c = enc_state_c, \n",
    "           batch_size = 2,\n",
    "           encoder = encoder, \n",
    "           decoder = decoder, \n",
    "           loss_function = loss_function, \n",
    "           optimizer = optimizer\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41602fd4-9b08-46e2-a4ad-2582e11c73d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
