{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f3fb4c2-6cc7-48f1-95da-16865ee4db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae5c1c7f-32ca-4db4-a20f-c24764eb1fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(string: str, \n",
    "               punctuations = r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "               stop_words = stopwords.words('english'),\n",
    "               # porter = PorterStemmer()\n",
    "               wnl = WordNetLemmatizer()\n",
    "              ):\n",
    "    \"\"\"\n",
    "    A method to clean text. It removes punctuations, stop words, applies lemmatization.\n",
    "    \"\"\"\n",
    "    # Removing the punctuations\n",
    "    for x in string.lower(): \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\") \n",
    "\n",
    "    # Converting the text to lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Removing stop words\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    # stemming/lemmatizing words. That means changing word to its basic format, for example\n",
    "    # words 'fishing', 'fished', 'fischer' will be changed into a word 'fisch'\n",
    "    # lemmatization should be better because stemming changes words too much, for example\n",
    "    # business is changed into busi\n",
    "    # string = ' '.join([porter.stem(word) for word in string.split()])\n",
    "    string = ' '.join([wnl.lemmatize(word, pos = \"v\") for word in string.split()])\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "    return string\n",
    "\n",
    "def create_training_data(tokenizer,\n",
    "                         sentences_file,\n",
    "                         embed_matrix_file,\n",
    "                         model_folder,\n",
    "                         max_sen_len = None\n",
    "                        ):\n",
    "    \"\"\"\n",
    "    Creating a training and testing datasets self.x_train, self.x_test, self.y_train, self.y_test. This function\n",
    "    also creates and saves a tokenizer and a list of all unique tables names all_unique_values because when we load\n",
    "    a ready model those values are needed for the 'predict' function.\n",
    "    \"\"\"\n",
    "    sentences_tables = pd.read_excel(sentences_file).values\n",
    "    random.shuffle(sentences_tables)\n",
    "    clean_sentences = np.array([clean_text(sentence) for sentence in sentences_tables[:, 0]])\n",
    "\n",
    "    tokenizer.fit_on_texts(clean_sentences)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(clean_sentences)\n",
    "    if max_sen_len == None:\n",
    "        max_sen_len = np.max([len(seq) for seq in sequences])\n",
    "    x = pad_sequences(sequences, maxlen = max_sen_len)\n",
    "\n",
    "    embed_matrix = pd.read_csv(embed_matrix_file).values\n",
    "\n",
    "    x_train, x_test = train_test_split(x, test_size = 0.2)\n",
    "\n",
    "    with open(os.path.join(model_folder, 'tokenizer.json'), 'w') as file:\n",
    "        json.dump(tokenizer.to_json(), file)\n",
    "        \n",
    "    return x_train, x_test\n",
    "\n",
    "\n",
    "def get_coefs(word, *arr): \n",
    "    return word, list(np.asarray(arr, dtype='float'))\n",
    "\n",
    "\n",
    "def create_embedding_file(tokenizer,\n",
    "                          embed_file_src = r'model\\glove.840B.300d.txt', \n",
    "                          embed_file_trg = r'model\\model_embeddings.txt'\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    This function will create an embedding file called embed_file_trg which will contain only those words \n",
    "    from embed_file_src which are present in the training dataset (tokenizer.word_index).\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings = dict(get_coefs(*o.split(\" \")) for o in open(embed_file_src, errors = 'ignore'))\n",
    "    with open(embed_file_trg, 'w') as file:\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            word_vector = embeddings[word]\n",
    "            line = ' '.join(np.concatenate([[word], word_vector]))\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "\n",
    "def create_embedding_matrix(tokenizer,\n",
    "                            model_folder,\n",
    "                            word_vec_dim,\n",
    "                            embed_file_path,\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    A function to create an embedding matrix. This is a matrix where each row is a vector representing a word.\n",
    "    To create that matrix we use a word embedding file which path is equal to embedding_file_path.\n",
    "    embedding_matrix[row_number] is a vector representation for a word = list(tokenizer.word_index.keys())[row_number - 1]\n",
    "    First row of embedding_matrix are zeros. This matrix is needed to train a model.\n",
    "    \"\"\"\n",
    "    embeddings = dict(get_coefs(*o.split(\" \")) for o in open(embed_file_path, errors = 'ignore'))\n",
    "\n",
    "    # embedding_matrix[row_number] is a vector representation of a word = self.tokenizer.word_index.keys()[row_number - 1]\n",
    "    # first row in embedding_matrix is 0\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_counts) + 1, word_vec_dim))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index > len(tokenizer.word_counts):\n",
    "            break\n",
    "        else:\n",
    "            try:\n",
    "                embedding_matrix[index] = embeddings[word]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    pd.DataFrame(embedding_matrix).to_csv(os.path.join(model_folder, 'embedding_matrix.csv'))\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a6c030-6b64-4d53-9cdf-18ae47d5b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# max_sen_len = 20\n",
    "sentences_file = r'data\\sentences_tables.xlsx'\n",
    "embed_matrix_file = r'model\\embedding_matrix.csv'\n",
    "model_folder = 'model'\n",
    "word_vec_dim = 300\n",
    "embed_file_path = r'model\\model_embeddings.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa21b414-1a9d-4045-b551-9416b28cf16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = create_training_data(\n",
    "    tokenizer = tokenizer, \n",
    "    # max_sen_len = max_sen_len,\n",
    "    sentences_file = sentences_file,\n",
    "    embed_matrix_file = embed_matrix_file,\n",
    "    model_folder = model_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "703325ad-5693-45fb-8ca5-0632a439fcfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  5, 13,  6],\n",
       "       [ 0,  0,  0, ...,  5,  6,  8],\n",
       "       [ 0,  0,  0, ...,  5,  6, 13],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  2, 21, 10],\n",
       "       [ 0,  0,  0, ...,  1, 26, 11],\n",
       "       [ 0,  0,  0, ...,  3, 27, 12]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fbd939a-fe1a-4b2d-9b3b-b46858c3282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_matrix = create_embedding_matrix(\n",
    "    tokenizer = tokenizer,\n",
    "    model_folder = model_folder,\n",
    "    word_vec_dim = word_vec_dim,\n",
    "    embed_file_path = embed_file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a69efdd-4542-4dfc-8cb3-d65237cdf61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [-0.50318 ,  0.27905 , -0.045497, ...,  0.4781  ,  0.13005 ,\n",
       "        -0.014399],\n",
       "       [-0.89423 ,  0.39636 ,  0.64359 , ..., -0.15076 ,  0.06987 ,\n",
       "         0.041258],\n",
       "       ...,\n",
       "       [ 0.37492 , -0.052425, -0.60094 , ..., -0.36104 , -0.065253,\n",
       "        -0.1206  ],\n",
       "       [ 0.012832,  0.22669 , -0.17511 , ...,  0.17134 ,  0.040047,\n",
       "        -0.37131 ],\n",
       "       [-0.39054 , -0.55117 , -0.073466, ...,  0.34569 ,  0.30918 ,\n",
       "        -0.32873 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8854e78-9e2f-4fde-973c-209d545335c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('employee', 1)\n",
      "('cost', 2)\n",
      "('user', 3)\n",
      "('office', 4)\n",
      "('business', 5)\n",
      "('unit', 6)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(tokenizer.word_index.items()):\n",
    "    print(item)\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "130632c9-bbf4-4a09-a7e1-e8c153935bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self,\n",
    "                 embedding_dim,\n",
    "                 lstm_out_size,\n",
    "                 batch_size,\n",
    "                 embed_matrix\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.lstm_out_size = lstm_out_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = layers.Embedding(\n",
    "            input_dim = embed_matrix.shape[0],\n",
    "            output_dim = embedding_dim,\n",
    "            embeddings_initializer = tf.keras.initializers.Constant(embed_matrix),\n",
    "            trainable = False\n",
    "        )\n",
    "        self.lstm = layers.LSTM(\n",
    "            units = self.lstm_out_size,\n",
    "            return_sequences = True,\n",
    "            return_state = True\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def call(self, x, state_h = None, state_c = None):\n",
    "        # x.shape = (batch_size, max_sen_len)\n",
    "        # x is a series of numbers which represent words\n",
    "        # state_h.shape = (batch_size, lstm_out_size)\n",
    "        \n",
    "        if state_h == None or state_c == None:\n",
    "            state_h, state_c = self.initialize_hidden_state()\n",
    "        \n",
    "        # make sure that the types are correct\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        state_h = tf.cast(state_h, tf.float32)\n",
    "        state_c = tf.cast(state_c, tf.float32)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        # x.shape after embedding = (batch_size, max_sen_len, embedding_dim)\n",
    "        # output.shape = (batch_size, max_sen_len, lstm_out_size)\n",
    "        # state_h.shape = (batch_size, lstm_out_size)\n",
    "        output, state_h, state_c = self.lstm(x, initial_state = [state_h, state_c])\n",
    "        return output, state_h, state_c\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        state_h = tf.zeros((self.batch_size, self.lstm_out_size))\n",
    "        state_c = tf.zeros((self.batch_size, self.lstm_out_size))\n",
    "        return state_h, state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e07b1daa-cfc1-4cfe-85a6-a58ce5120f4f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(300, 40), dtype=float32, numpy=\n",
       " array([[-0.13181046,  0.17578174, -0.2120846 , ..., -0.13128494,\n",
       "         -0.16249536, -0.21907593],\n",
       "        [ 0.06678414, -0.09048228,  0.1054726 , ...,  0.06102577,\n",
       "          0.07817639,  0.10394341],\n",
       "        [ 0.03403007, -0.03505441,  0.06919112, ...,  0.0738593 ,\n",
       "          0.07218578,  0.10790002],\n",
       "        ...,\n",
       "        [ 0.06178866, -0.0966633 ,  0.07948293, ...,  0.00635299,\n",
       "          0.03442178,  0.03179796],\n",
       "        [ 0.02383993, -0.03409163,  0.03514545, ...,  0.01484937,\n",
       "          0.02266026,  0.02819575],\n",
       "        [ 0.00046793,  0.00032897,  0.002085  , ...,  0.00415377,\n",
       "          0.00336663,  0.00551508]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(10, 40), dtype=float32, numpy=\n",
       " array([[ 0.00786627, -0.00872229,  0.01512839,  0.00888416, -0.01922235,\n",
       "          0.00786126, -0.00542759,  0.00856517,  0.00329846,  0.01680919,\n",
       "          0.00409558, -0.00518857,  0.00704846,  0.0061653 , -0.00591288,\n",
       "          0.01023573,  0.00253037,  0.00250717,  0.0043835 ,  0.00831162,\n",
       "          0.05719082,  0.0236755 ,  0.03888982,  0.03290932,  0.03245506,\n",
       "          0.06420724,  0.02376345,  0.06240852,  0.04213234,  0.03568809,\n",
       "          0.02032051, -0.02741202,  0.02445441,  0.02712747, -0.02703562,\n",
       "          0.01896605,  0.00131836,  0.01467683,  0.01487345,  0.02186352],\n",
       "        [-0.01076528,  0.01193678, -0.02070378, -0.01215831,  0.02630653,\n",
       "         -0.01075843,  0.00742787, -0.01172176, -0.00451406, -0.02300403,\n",
       "         -0.00560496,  0.00710075, -0.00964609, -0.00843745,  0.008092  ,\n",
       "         -0.01400798, -0.00346291, -0.00343116, -0.00599899, -0.01137477,\n",
       "         -0.07826782, -0.03240082, -0.05322221, -0.04503766, -0.044416  ,\n",
       "         -0.08787006, -0.03252119, -0.08540844, -0.05765972, -0.04884051,\n",
       "         -0.02780939,  0.03751439, -0.0334668 , -0.03712498,  0.03699928,\n",
       "         -0.02595577, -0.00180423, -0.0200858 , -0.02035489, -0.02992106],\n",
       "        [ 0.00772667, -0.00856751,  0.01485993,  0.00872651, -0.01888124,\n",
       "          0.00772176, -0.00533128,  0.00841318,  0.00323992,  0.01651091,\n",
       "          0.0040229 , -0.00509649,  0.00692338,  0.00605589, -0.00580795,\n",
       "          0.01005409,  0.00248547,  0.00246268,  0.00430572,  0.00816412,\n",
       "          0.05617594,  0.02325537,  0.03819971,  0.03232533,  0.03187913,\n",
       "          0.06306785,  0.02334176,  0.06130105,  0.04138468,  0.03505478,\n",
       "          0.01995991, -0.02692558,  0.02402046,  0.02664608, -0.02655586,\n",
       "          0.01862949,  0.00129497,  0.01441638,  0.01460952,  0.02147554],\n",
       "        [ 0.01218865, -0.01351504,  0.02344119,  0.01376586, -0.02978472,\n",
       "          0.01218089, -0.00840996,  0.01327158,  0.0051109 ,  0.02604556,\n",
       "          0.00634603, -0.0080396 ,  0.01092147,  0.00955303, -0.00916191,\n",
       "          0.01586009,  0.00392077,  0.00388482,  0.00679216,  0.01287871,\n",
       "          0.08861621,  0.03668479,  0.06025913,  0.05099244,  0.05028858,\n",
       "          0.09948804,  0.03682107,  0.09670095,  0.06528335,  0.05529809,\n",
       "          0.03148629, -0.04247446,  0.0378917 ,  0.04203356, -0.04189124,\n",
       "          0.02938758,  0.00204278,  0.0227415 ,  0.02304616,  0.03387715],\n",
       "        [-0.01012194,  0.01122343, -0.01946651, -0.01143173,  0.02473443,\n",
       "         -0.0101155 ,  0.00698397, -0.01102126, -0.0042443 , -0.02162929,\n",
       "         -0.00527   ,  0.00667641, -0.00906963, -0.00793322,  0.00760842,\n",
       "         -0.01317086, -0.00325597, -0.00322611, -0.00564049, -0.010695  ,\n",
       "         -0.07359049, -0.03046453, -0.05004162, -0.04234618, -0.04176167,\n",
       "         -0.08261889, -0.03057771, -0.08030438, -0.05421394, -0.04592177,\n",
       "         -0.02614749,  0.03527251, -0.0314668 , -0.03490636,  0.03478818,\n",
       "         -0.02440464, -0.00169641, -0.01888546, -0.01913847, -0.02813296],\n",
       "        [ 0.01675388, -0.01857707,  0.03222105,  0.01892184, -0.04094053,\n",
       "          0.01674322, -0.0115599 ,  0.01824243,  0.00702518,  0.03580089,\n",
       "          0.00872293, -0.01105081,  0.01501209,  0.0131311 , -0.01259349,\n",
       "          0.02180046,  0.00538929,  0.00533987,  0.00933616,  0.01770241,\n",
       "          0.12180726,  0.05042501,  0.08282908,  0.07009156,  0.06912408,\n",
       "          0.1367511 ,  0.05061234,  0.13292013,  0.08973511,  0.07600989,\n",
       "          0.04327942, -0.0583832 ,  0.05208398,  0.05777715, -0.05758153,\n",
       "          0.04039465,  0.0028079 ,  0.03125929,  0.03167807,  0.04656578],\n",
       "        [ 0.0034023 , -0.00377255,  0.0065433 ,  0.00384256, -0.00831401,\n",
       "          0.00340014, -0.00234753,  0.00370459,  0.00142664,  0.00727028,\n",
       "          0.00177141, -0.00224415,  0.00304859,  0.0026666 , -0.00255743,\n",
       "          0.00442714,  0.00109443,  0.0010844 ,  0.00189594,  0.00359492,\n",
       "          0.02473606,  0.01024008,  0.01682055,  0.01423387,  0.0140374 ,\n",
       "          0.02777079,  0.01027812,  0.02699281,  0.018223  ,  0.01543574,\n",
       "          0.00878899, -0.01185619,  0.01057697,  0.01173312, -0.01169339,\n",
       "          0.00820316,  0.00057022,  0.00634799,  0.00643304,  0.00945636],\n",
       "        [ 0.00289028, -0.00320481,  0.00555859,  0.00326429, -0.00706283,\n",
       "          0.00288844, -0.00199425,  0.00314708,  0.00121194,  0.00617616,\n",
       "          0.00150483, -0.00190642,  0.0025898 ,  0.0022653 , -0.00217256,\n",
       "          0.00376089,  0.00092973,  0.0009212 ,  0.00161062,  0.00305392,\n",
       "          0.02101349,  0.00869904,  0.0142892 ,  0.0120918 ,  0.01192489,\n",
       "          0.02359152,  0.00873135,  0.02293062,  0.01548059,  0.01311279,\n",
       "          0.00746632, -0.01007194,  0.00898523,  0.00996738, -0.00993364,\n",
       "          0.00696866,  0.0004844 ,  0.00539268,  0.00546492,  0.00803326],\n",
       "        [ 0.00623035, -0.00690835,  0.0119822 ,  0.00703656, -0.01522476,\n",
       "          0.00622638, -0.00429884,  0.0067839 ,  0.00261249,  0.01331345,\n",
       "          0.00324384, -0.00410952,  0.00558262,  0.00488313, -0.0046832 ,\n",
       "          0.00810704,  0.00200414,  0.00198576,  0.00347188,  0.00658308,\n",
       "          0.04529706,  0.0187518 ,  0.03080206,  0.02606529,  0.02570551,\n",
       "          0.05085431,  0.01882146,  0.04942966,  0.03337024,  0.02826617,\n",
       "          0.01609453, -0.02171125,  0.01936872,  0.02148587, -0.02141313,\n",
       "          0.01502176,  0.00104419,  0.01162455,  0.01178028,  0.01731665],\n",
       "        [ 0.01220214, -0.01353   ,  0.02346714,  0.01378109, -0.02981769,\n",
       "          0.01219437, -0.00841927,  0.01328627,  0.00511656,  0.02607439,\n",
       "          0.00635306, -0.0080485 ,  0.01093356,  0.0095636 , -0.00917205,\n",
       "          0.01587764,  0.00392511,  0.00388912,  0.00679968,  0.01289297,\n",
       "          0.0887143 ,  0.03672539,  0.06032583,  0.05104888,  0.05034424,\n",
       "          0.09959816,  0.03686183,  0.09680799,  0.06535561,  0.0553593 ,\n",
       "          0.03152114, -0.04252148,  0.03793365,  0.04208008, -0.04193761,\n",
       "          0.02942011,  0.00204504,  0.02276668,  0.02307167,  0.03391465]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(40,), dtype=float32, numpy=\n",
       " array([ 0.21266335, -0.29468614,  0.32669115,  0.29646307, -0.37095505,\n",
       "         0.47472435,  0.03590443,  0.14223471,  0.19071302,  0.31787902,\n",
       "         0.03302251, -0.04183522,  0.05683148,  0.04971059, -0.04767534,\n",
       "         0.08253029,  0.0204023 ,  0.02021522,  0.03534402,  0.06701626,\n",
       "         1.4891177 ,  0.4326045 ,  0.8738692 ,  0.668535  ,  1.0939173 ,\n",
       "         1.0535567 ,  1.2421395 ,  1.3478371 ,  0.83105725,  1.5562631 ,\n",
       "         0.27811497, -0.38838986,  0.33951867,  0.3828947 , -0.32269895,\n",
       "         0.32340723,  0.06264355,  0.16894266,  0.22973695,  0.29838187],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(embedding_dim = 300,\n",
    "                 lstm_out_size = 10,\n",
    "                 batch_size = 2,\n",
    "                 embed_matrix = embed_matrix\n",
    "                 )\n",
    "\n",
    "x = np.array([[1, 2], [1, 2]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    output, state_h, state_c = encoder(x)\n",
    "    \n",
    "variables = encoder.trainable_variables\n",
    "gradients = tape.gradient(output, variables)\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e586409-6d87-463c-90dd-40135c565e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1 like in Jonathan Hui pdf\n",
    "class Bahdau_attention(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.W2 = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "        \n",
    "    def call(self, decoder_hidden, encoder_hidden):\n",
    "        # decoder_hidden.shape = (batch_size, hidden_size)\n",
    "        # decoder_hidden_time_axis.shape = (batch_size, 1, hidden_size)\n",
    "        decoder_hidden_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
    "        \n",
    "        # encoder_hidden.shape = (batch_size, max_sen_len, hidden_size)\n",
    "        # argument for tanh shape = (batch_size, max_sen_len, hidden_size)\n",
    "        # score.shape = (batch_size, max_sen_len, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(decoder_hidden_time_axis) + self.W2(encoder_hidden)))\n",
    "        \n",
    "        # attention_weights.shape = (batch_size, max_sen_len, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis = 1)\n",
    "        \n",
    "        # context_vector.shape = (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * encoder_hidden\n",
    "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "398014c9-b9ab-4f29-84d1-7c96af06a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2, like in attention explanation pdf\n",
    "class Bahdau_attention(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = layers.Dense(1)\n",
    "        \n",
    "    def call(self, decoder_state_h, encoder_states_h):\n",
    "        # decoder_state_h.shape = (batch_size, dec_state_size)\n",
    "        # encoder_states_h.shape = (batch_size, max_sen_len, enc_state_size)\n",
    "        \n",
    "        # make sure that the dtypes are correct\n",
    "        decoder_state_h = tf.cast(decoder_state_h, tf.float32)\n",
    "        encoder_states_h = tf.cast(encoder_states_h, tf.float32)\n",
    "        \n",
    "        # encoder_states_h_flattened.shape = (max_sen_len, enc_state_size)\n",
    "        encoder_states_h_flattened = tf.reshape(encoder_states_h, [-1, tf.shape(encoder_states_h)[2]])\n",
    "        batch_size, _, enc_state_size = tf.shape(encoder_states_h)\n",
    "        \n",
    "        max_sen_len = encoder_states_h.shape[1]\n",
    "        for i in range(batch_size):\n",
    "            attention_weights = tf.constant([])\n",
    "            for j in range(max_sen_len):\n",
    "                e = tf.constant([])\n",
    "                for k in range(max_sen_len):\n",
    "                    x = tf.concat([decoder_state_h[i], encoder_states_h_flattened[i + k]], 0)\n",
    "                    # x.shape = (dec_state_size + enc_state_size)\n",
    "                    x = tf.expand_dims(x, 0)\n",
    "                    # x.shape = (1, dec_state_size + enc_state_size)\n",
    "                    e = tf.experimental.numpy.append(e, tf.math.exp(self.dense(x)))\n",
    "\n",
    "                new_attention_weight = tf.math.divide(e[j], tf.math.reduce_sum(e))\n",
    "                attention_weights = tf.experimental.numpy.append(attention_weights, new_attention_weight)\n",
    "                \n",
    "            # context_vector.shape = (batch_size, enc_state_size)\n",
    "            new_context_vector_row = tf.reduce_sum([attention_weights[j] * encoder_states_h_flattened[i + j] for j in range(max_sen_len)], axis = 0)\n",
    "            new_context_vector_row = tf.expand_dims(new_context_vector_row, 0)\n",
    "            if i == 0:\n",
    "                context_vector = new_context_vector_row\n",
    "            else:\n",
    "                context_vector = tf.concat([context_vector, new_context_vector_row], axis = 0)\n",
    "        \n",
    "        return tf.cast(context_vector, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a9ec3a1b-a11a-496d-8cb8-95960056ab17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[ 5.4463744e-06],\n",
       "        [ 6.8396330e-06],\n",
       "        [ 8.2999468e-06],\n",
       "        [-4.8494557e-01],\n",
       "        [-4.8494416e-01]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.4230609e-06], dtype=float32)>]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = Bahdau_attention()\n",
    "decoder_state_h = tf.constant([[1,2,3], [4,5,6]])\n",
    "encoder_states_h = tf.constant([[[1,2], [4,5]], [[1,2], [4,5]]])\n",
    "encoder_states_h_flattened = tf.reshape(encoder_states_h, [-1, tf.shape(encoder_states_h)[2]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    context_vector = attention(decoder_state_h, encoder_states_h)\n",
    "    \n",
    "variables = attention.trainable_variables\n",
    "gradients = tape.gradient(context_vector, variables)\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a40358e6-54e8-41a7-b551-131a2d5f8922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[4.872717 , 6.8218036],\n",
       "       [4.8727164, 6.821803 ]], dtype=float32)>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "15a6c0cc-731a-475c-bc69-bea3475e4fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_out_size, embed_matrix):\n",
    "        super().__init__()\n",
    "        self.lstm_out_size = lstm_out_size\n",
    "        self.embedding = layers.Embedding(\n",
    "            input_dim = embed_matrix.shape[0],\n",
    "            output_dim = embedding_dim,\n",
    "            embeddings_initializer = tf.keras.initializers.Constant(embed_matrix),\n",
    "            trainable = False\n",
    "        )\n",
    "        self.lstm = layers.LSTM(\n",
    "            units = self.lstm_out_size,\n",
    "            # return_sequences = True,\n",
    "            return_state = True\n",
    "        )\n",
    "        self.dense = layers.Dense(vocab_size)\n",
    "        self.attention = Bahdau_attention()\n",
    "        \n",
    "    def call(self, x, decoder_state_h, decoder_state_c, encoder_states_h):\n",
    "        # x.shape = (batch_size, 1)\n",
    "        # x is a single number for each batch representing a single word\n",
    "        # encoder_states_h.shape = (batch_size, max_sen_len, enc_state_size)\n",
    "        # decoder_state_h.shape = (batch_size, lstm_out_size)\n",
    "        \n",
    "        # make sure that the types are correct\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        decoder_state_h = tf.cast(decoder_state_h, tf.float32)\n",
    "        decoder_state_c = tf.cast(decoder_state_c, tf.float32)\n",
    "        encoder_states_h = tf.cast(encoder_states_h, tf.float32)\n",
    "        \n",
    "        # context_vector.shape = (batch_size, enc_state_size)\n",
    "        context_vector = self.attention(decoder_state_h, encoder_states_h)\n",
    "        # shape of output of embedding layer = (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        # x.shape after concatenation = (batch_size, 1, enc_state_size + embedding_dim)\n",
    "        # print('context_vector: ', tf.expand_dims(context_vector, 1))\n",
    "        # print('x: ', x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = 2)\n",
    "        \n",
    "        output, state_h, state_c = self.lstm(x, initial_state = [decoder_state_h, decoder_state_c])\n",
    "        \n",
    "        # output.shape = (batch_size, vocab_size)\n",
    "        output = self.dense(output)\n",
    "        \n",
    "        return output, state_h, state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0bfb23f3-bb93-4967-9b92-8bdf3247ab4e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vector:  tf.Tensor(\n",
      "[[[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14.]]\n",
      "\n",
      " [[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14.]]], shape=(2, 1, 15), dtype=float32)\n",
      "x:  tf.Tensor(\n",
      "[[[-5.0318e-01  2.7905e-01 -4.5497e-02  8.7287e-02  8.2939e-03\n",
      "    8.1533e-02 -2.0737e-01 -6.5505e-01  1.3109e-01  3.1086e+00\n",
      "   -6.5408e-01 -5.5146e-02 -5.9301e-01  2.5131e-01  1.4342e-01\n",
      "    7.9765e-02 -1.6456e-01  1.0091e+00 -3.7291e-01 -7.6170e-02\n",
      "    8.7821e-02  1.4089e-01 -7.8971e-02 -3.5242e-01  6.3374e-01\n",
      "    2.0048e-01 -4.1246e-01  2.9199e-02  5.7455e-01 -2.2509e-01\n",
      "   -5.6851e-02 -5.9095e-01 -7.1849e-02  1.1489e-01 -2.7550e-01\n",
      "    1.1122e-01  1.5333e-01  8.6889e-02 -5.3122e-01 -2.6919e-01\n",
      "    4.1412e-01 -3.8239e-02  3.6797e-01  3.5596e-01 -1.4596e-01\n",
      "   -2.4261e-01 -4.5774e-01 -8.9788e-02  3.1969e-01  4.3052e-01\n",
      "    5.7857e-03 -6.8859e-01  2.3270e-01  4.1043e-02 -4.0725e-01\n",
      "    1.1305e-01  2.8135e-01  3.4865e-01 -9.3155e-02 -2.0771e-01\n",
      "    3.3811e-01 -5.7351e-01  3.9391e-01  1.9663e-01  4.5935e-01\n",
      "   -3.7470e-01  2.5238e-01  6.3089e-01 -1.6333e-01  3.9450e-01\n",
      "   -1.8736e-01 -1.0258e-02  2.8220e-02  2.7290e-01  3.6584e-01\n",
      "    2.6755e-01  6.8216e-01 -3.2447e-02  3.0782e-01  2.6938e-01\n",
      "    1.7032e-01 -2.7999e-01  1.0562e-01 -3.1781e-01 -1.2182e-01\n",
      "    6.7313e-01  2.6754e-01  3.3644e-01  5.1560e-02 -2.0461e-01\n",
      "   -1.0551e-01 -6.7275e-02 -1.2480e-01 -4.1668e-01  1.4401e-01\n",
      "   -2.9921e-01 -3.6734e-01 -2.7202e-01  3.0255e-01  3.2865e-01\n",
      "    3.7059e-01  1.1103e-02  4.4558e-01  2.7701e-01 -5.8865e-01\n",
      "   -1.2326e+00  3.7588e-01  3.4407e-01 -2.3389e-01 -1.6041e-01\n",
      "   -1.8474e-01  4.8864e-01 -3.1429e-01  4.8102e-01  1.3882e-02\n",
      "    6.9906e-02  3.6818e-01 -3.6117e-01 -1.4200e-02  2.7700e-01\n",
      "    2.5202e-01 -1.2017e-01  3.5121e-01 -3.4728e-02 -2.6642e-01\n",
      "    2.9228e-01 -1.3809e-01  1.4551e-01 -6.9742e-01 -5.5339e-02\n",
      "   -3.7245e-01  2.0986e-01 -7.0773e-02 -4.5994e-01 -6.2296e-02\n",
      "    2.7484e-01 -1.3918e-01  4.1060e-02 -4.5529e-01  1.8242e-01\n",
      "    6.0428e-01 -7.7745e-02  1.1230e-01  1.7492e-01 -8.1565e-01\n",
      "   -1.9184e-01  2.7888e-01 -2.9558e-02 -5.2377e-02 -5.8775e-01\n",
      "   -2.0615e-01 -2.9161e-01 -3.8812e-01  8.0060e-02  3.6030e-01\n",
      "    5.8070e-01  2.1867e-01  3.4190e-01  1.9917e-01 -2.9075e-01\n",
      "   -5.7359e-01  3.9910e-02  3.6946e-01 -4.1165e-01 -4.8416e-01\n",
      "    2.8805e-04  6.7215e-02  2.6788e-01 -3.0540e-02  7.2055e-03\n",
      "   -1.3216e-01 -3.1577e-01  3.7059e-02 -6.4242e-02 -4.0034e-03\n",
      "    5.6804e-01 -1.6890e-01  3.9321e-01 -2.0985e-01  2.7154e-01\n",
      "    2.6393e-01  1.8300e-01 -1.1818e-01 -1.6989e-02 -1.2728e-01\n",
      "    9.2201e-01 -1.3046e-01  3.9582e-01  2.3773e-01 -5.4231e-02\n",
      "   -2.5861e-01  3.5903e-01  9.6424e-03  7.1228e-02  4.9206e-01\n",
      "    1.8449e-01  6.8007e-01  2.8893e-02  2.7795e-01  2.8300e-01\n",
      "    4.7372e-01 -3.3998e-01 -1.4885e-01  1.2910e-01 -5.2419e-01\n",
      "   -1.2858e-01  3.6505e-01  4.3187e-01  6.5505e-01 -6.1118e-02\n",
      "   -5.4731e-02  2.3029e-01 -3.4115e-01  5.5840e-01 -1.4857e-01\n",
      "    3.0603e-01  1.1652e-02 -3.1424e-01 -2.3531e-01 -2.2034e-01\n",
      "   -4.0319e-01  2.7851e-01 -2.5669e-01  6.3985e-01 -2.2065e-01\n",
      "   -3.4301e-01  3.5846e-01 -4.4597e-02  8.8124e-02 -7.0686e-01\n",
      "   -5.1382e-01 -1.1983e-02 -8.3112e-02 -2.3997e-01  2.0922e-01\n",
      "   -1.1523e-01  1.5612e-01 -1.4251e-01 -2.0092e-01  4.7835e-01\n",
      "    2.3283e-01 -1.7265e-01  6.9838e-01  1.2568e-01  4.3191e-01\n",
      "    5.4816e-02 -5.0084e-02 -3.1167e-01 -2.6080e-03 -3.0068e-01\n",
      "    4.2556e-02  4.8408e-01 -1.6999e-01  3.3080e-02 -2.5067e-01\n",
      "    1.4472e-01 -1.7902e-01 -1.8851e-01  2.2440e-01  1.9191e-01\n",
      "   -2.4927e-01  2.8459e-01 -4.8320e-02 -3.7724e-03 -1.3070e-01\n",
      "    1.8643e-01 -5.4704e-02 -1.4585e-01  4.5038e-01 -4.9506e-01\n",
      "    4.2250e-01 -5.0313e-01  4.0168e-01 -9.2121e-02 -2.8398e-02\n",
      "    1.8757e-03 -7.8517e-01 -4.6842e-04  3.4009e-01 -7.2495e-02\n",
      "   -3.3707e-01  1.3297e-01  5.7056e-02  7.5623e-01 -7.6904e-02\n",
      "   -5.0877e-02 -1.6279e-01  5.1903e-01 -1.9025e-02 -5.2407e-02\n",
      "   -2.1299e-01  4.1439e-01  8.7612e-01  4.3061e-01  1.1811e-01\n",
      "   -3.1186e-01 -1.3624e-02  4.7810e-01  1.3005e-01 -1.4399e-02]]\n",
      "\n",
      " [[-8.9423e-01  3.9636e-01  6.4359e-01 -1.9608e-01 -9.5566e-02\n",
      "   -5.3264e-01 -1.5511e-01 -2.3564e-01 -6.3772e-03  2.2256e+00\n",
      "   -3.4791e-01  3.2009e-01 -1.6021e-01  4.5511e-01 -1.1815e-01\n",
      "    5.1277e-02 -7.3197e-02  2.1911e+00 -1.9940e-01 -2.7191e-02\n",
      "   -4.1830e-02  4.3831e-01 -2.1534e-01 -4.3437e-01 -3.7689e-01\n",
      "   -2.3007e-01 -2.0907e-01 -3.1041e-01  4.0874e-01  2.6325e-01\n",
      "   -1.3956e-01 -1.8161e-01  2.8095e-01  4.5331e-01 -8.0134e-02\n",
      "    2.8115e-01 -4.6951e-03 -1.3229e-01  6.3280e-01  3.3249e-01\n",
      "   -9.2557e-03  5.5266e-01  3.1232e-01 -2.0404e-01 -1.4001e-01\n",
      "   -2.9673e-01 -6.2423e-02 -3.0363e-01  1.7869e-01  5.5295e-01\n",
      "    2.9335e-02 -2.8909e-01  5.1924e-02  1.0548e-01  3.6091e-01\n",
      "   -2.8289e-01  6.6317e-02  1.8614e-02 -1.7009e-01  1.7538e-01\n",
      "   -1.2912e-01 -5.9645e-01 -8.8556e-02  1.3846e-01  6.2411e-01\n",
      "    1.3170e-01 -3.6330e-02  4.2922e-01 -3.1901e-01 -3.4663e-01\n",
      "    2.1321e-01 -7.1668e-02  2.4049e-01 -1.7282e-01  4.4380e-01\n",
      "    1.5908e-03  2.9684e-01 -7.3339e-02  1.7626e-01  3.9140e-01\n",
      "    1.1625e-01  7.3711e-02  3.6605e-01  1.9746e-01 -1.0949e-01\n",
      "   -1.4512e-01  1.1712e-01  2.3329e-01  6.4055e-01 -2.3253e-01\n",
      "   -1.5162e-01  9.7202e-02 -3.8631e-01 -2.0369e-01 -4.3626e-01\n",
      "    1.4401e-01  9.5221e-02  1.2676e-01 -1.2967e-01  2.3122e-01\n",
      "   -1.3771e-01  1.2556e-01 -4.9413e-01 -6.0072e-02 -1.0405e-03\n",
      "   -1.4455e+00  2.0039e-01  3.5395e-01 -7.1023e-01 -8.9061e-02\n",
      "   -4.9561e-01 -7.8549e-01  3.6751e-01  1.9272e-02  1.7119e-01\n",
      "   -2.5892e-01 -2.0059e-02  1.4242e-01  1.2457e-01  1.1954e-01\n",
      "    2.4252e-01  2.9205e-02 -2.4668e-01 -3.4273e-01 -5.0957e-01\n",
      "   -3.7601e-01 -1.5000e-01 -9.3635e-02 -3.0062e-01 -1.7011e-01\n",
      "    3.5144e-01 -1.8873e-01 -3.3959e-02 -1.1912e-01  2.9050e-01\n",
      "   -1.1251e-01 -1.2751e-01  1.5233e-01  2.3064e-01 -1.5738e-01\n",
      "   -1.8113e-01  3.1169e-01  5.1011e-01  1.3914e-01 -1.2232e-01\n",
      "   -7.2347e-02 -7.6693e-02  2.2046e-01  5.9636e-01  1.0442e-01\n",
      "    2.4813e-01  2.8229e-01  6.6187e-02 -4.2278e-01  4.4155e-01\n",
      "   -8.4932e-02  2.6796e-01  4.2568e-01 -1.1185e-01  4.1953e-02\n",
      "   -2.3627e-01  6.9820e-01 -1.8663e-01 -6.2204e-01  2.0687e-01\n",
      "    2.7720e-01  6.9175e-01  1.9757e-01  6.4335e-01  4.0969e-01\n",
      "   -4.2080e-01 -2.5089e-01 -1.3206e-01 -3.3423e-03 -3.5673e-01\n",
      "   -4.1668e-01  1.1433e-01  1.8372e-01  2.0743e-01  6.7091e-02\n",
      "    1.0823e-01 -4.7070e-01 -6.2871e-01  2.3806e-01 -4.0338e-01\n",
      "    1.7839e-01  2.9633e-01 -4.5738e-01  1.4548e-01  7.7642e-01\n",
      "    3.1373e-01  5.3348e-01  1.2992e-01  1.6573e-02 -2.2105e-01\n",
      "   -2.0062e-01  2.7719e-02  7.4989e-02  1.2996e-01  2.9033e-01\n",
      "    7.5920e-01 -5.1475e-01 -3.7171e-01 -1.9469e-01 -4.0800e-01\n",
      "   -1.5819e-01  1.4589e-02 -1.3309e-01  8.6035e-02  8.6605e-01\n",
      "   -1.6288e-01 -1.2820e-01 -2.2419e-01 -6.5432e-02  5.8913e-01\n",
      "   -4.1833e-02 -2.9141e-01  5.9211e-01 -1.6994e-01  3.7035e-01\n",
      "   -3.4367e-01 -1.0973e-01 -1.6711e-01 -3.8910e-01  1.0490e-01\n",
      "   -4.7764e-02 -3.1205e-01  6.5650e-02 -2.9537e-01 -5.8174e-02\n",
      "    3.3941e-01 -2.4194e-01  2.1566e-01 -4.7461e-01 -4.2082e-02\n",
      "    3.0410e-01  4.3432e-01  1.3082e-01 -4.0839e-02  1.9183e-01\n",
      "    9.4233e-02  5.1361e-02 -9.6349e-02  1.0432e-01  4.1047e-01\n",
      "   -5.4162e-01  2.2467e-01 -4.7607e-01 -9.7617e-02 -2.0377e-01\n",
      "    4.1705e-01 -2.7179e-01  5.2780e-02 -2.7942e-01  2.0783e-01\n",
      "    4.5391e-01  9.3895e-02 -3.0409e-01 -1.8542e-01 -1.4475e-02\n",
      "    3.5039e-02  3.2353e-01  1.9472e-01  3.9531e-02 -1.0739e-01\n",
      "   -5.7708e-01  2.3655e-01  9.7729e-02  4.9531e-01  9.3618e-02\n",
      "    1.8399e-01 -1.6629e-01  2.0152e-01  1.6342e-01 -1.6451e-02\n",
      "    1.4544e-01 -1.2182e-01  3.1706e-01 -1.5215e-01  3.1301e-01\n",
      "   -9.4738e-02  7.2354e-02 -1.0180e-01  2.2357e-01 -2.0615e-01\n",
      "   -1.0655e-01  8.6504e-02 -1.9194e-01 -5.7293e-01  9.7392e-02\n",
      "   -5.7058e-01  7.6770e-02  1.8124e-01  1.7637e-01 -2.5672e-01\n",
      "    2.1281e-01 -7.1513e-02 -1.5076e-01  6.9870e-02  4.1258e-02]]], shape=(2, 1, 300), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_size = 100, \n",
    "                  embedding_dim = 300, \n",
    "                  lstm_out_size = 20, \n",
    "                  embed_matrix = embed_matrix\n",
    "                 )\n",
    "\n",
    "x = tf.constant([[1], [2]])\n",
    "decoder_state_h = tf.constant([[i for i in range(20)], [i for i in range(20)]])\n",
    "decoder_state_c = tf.constant([[i for i in range(20)], [i for i in range(20)]])\n",
    "encoder_states_h = tf.constant([[[i for i in range(15)], [i for i in range(15)]], [[i for i in range(15)], [i for i in range(15)]]])\n",
    "\n",
    "output, state_h, state_c = decoder(x, decoder_state_h, decoder_state_c, encoder_states_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "53ffc42e-fa8e-4ca6-956a-948ae38af438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inp, \n",
    "               targ, \n",
    "               # enc_state_h, \n",
    "               # enc_state_c, \n",
    "               batch_size, \n",
    "               encoder, \n",
    "               decoder, \n",
    "               loss_function, \n",
    "               optimizer):\n",
    "    # inp.shape = targ.shape (batch_size, max_sen_len)\n",
    "    # enc_state_h.shape = (batch_size, enc_state_size)\n",
    "    \n",
    "    # make sure that the types are correct\n",
    "    inp = tf.cast(inp, tf.float32)\n",
    "    targ = tf.cast(targ, tf.float32)\n",
    "    # enc_state_h = tf.cast(enc_state_h, tf.float32)\n",
    "    # enc_state_c = tf.cast(enc_state_c, tf.float32)\n",
    "    \n",
    "    batch_loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # enc_output.shape = (batch_size, max_sen_len, enc_state_size)\n",
    "        # enc_state_h.shape = (batch_size, state_size)\n",
    "        # enc_output, enc_state_h, enc_state_c = encoder(inp, enc_state_h, enc_state_c)\n",
    "        enc_output, enc_state_h, enc_state_c = encoder(inp)\n",
    "        dec_state_h = enc_state_h\n",
    "        dec_state_c = enc_state_c\n",
    "        \n",
    "        # dec_input.shape = (batch_size, 1)\n",
    "        dec_input = tf.expand_dims([0] * batch_size, 1)\n",
    "        \n",
    "        for t in range(targ.shape[1]):\n",
    "            prediction, dec_state_h, dec_state_c, = decoder(dec_input, dec_state_h, dec_state_c, enc_output)\n",
    "            # real value passed to loss_function needs to have shape (batch_size).\n",
    "            # It is a number representing a word from tokenizer.word_index. Real value = 0\n",
    "            # means that there was no word\n",
    "            batch_loss += loss_function(targ[:, t], prediction)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "            \n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(batch_loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "16035f6f-2d3d-43bd-8f41-174494e45259",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction = 'none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype = loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a8ddff7-2151-45af-9547-df2a605e46cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([5.1293328e-02, 1.1920928e-07], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# real = tf.expand_dims(x_train[:2, 0], 1)\n",
    "# pred = tf.expand_dims(x_train[:2, 0], 1)\n",
    "\n",
    "real = tf.constant([1, 0])\n",
    "pred = tf.constant([[0.05, 0.95], [1, 0]])\n",
    "\n",
    "real = tf.cast(real, tf.float32)\n",
    "pred = tf.cast(pred, tf.float32)\n",
    "\n",
    "loss_object(real, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "423aca49-8c11-4305-bb06-80e0091323dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 20\n",
    "embedding_dim = 300\n",
    "\n",
    "inp = tf.constant(x_train)\n",
    "targ = tf.constant(x_train)\n",
    "\n",
    "decoder = Decoder(vocab_size = len(tokenizer.word_index.keys()) + 1,\n",
    "                  embedding_dim = embedding_dim,\n",
    "                  lstm_out_size = 100,\n",
    "                  embed_matrix = embed_matrix\n",
    "                 )\n",
    "\n",
    "encoder = Encoder(embedding_dim = embedding_dim,\n",
    "                 lstm_out_size = 100,\n",
    "                 batch_size = batch_size,\n",
    "                 embed_matrix = embed_matrix\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "48974688-4e2c-4b76-b5ea-b065ea4adc39",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 0, Loss: 2.7100887298583984\n",
      "Time per batch: 175.22068905830383\n",
      "Batch number: 1, Loss: 2.647005796432495\n",
      "Time per batch: 182.91211879253387\n",
      "Batch number: 2, Loss: 2.696329116821289\n",
      "Time per batch: 186.9434502919515\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [124], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m inp_batch \u001b[38;5;241m=\u001b[39m inp[batch_number \u001b[38;5;241m*\u001b[39m batch_size : (batch_number \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_size, :]\n\u001b[0;32m      6\u001b[0m targ_batch \u001b[38;5;241m=\u001b[39m targ[batch_number \u001b[38;5;241m*\u001b[39m batch_size : (batch_number \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_size, :]\n\u001b[1;32m----> 8\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m train_step(inp \u001b[38;5;241m=\u001b[39m inp_batch, \n\u001b[0;32m      9\u001b[0m                        targ \u001b[38;5;241m=\u001b[39m targ_batch,\n\u001b[0;32m     10\u001b[0m                        batch_size \u001b[38;5;241m=\u001b[39m batch_size,\n\u001b[0;32m     11\u001b[0m                        encoder \u001b[38;5;241m=\u001b[39m encoder, \n\u001b[0;32m     12\u001b[0m                        decoder \u001b[38;5;241m=\u001b[39m decoder, \n\u001b[0;32m     13\u001b[0m                        loss_function \u001b[38;5;241m=\u001b[39m loss_function, \n\u001b[0;32m     14\u001b[0m                        optimizer \u001b[38;5;241m=\u001b[39m optimizer\n\u001b[0;32m     15\u001b[0m                       )\n\u001b[0;32m     16\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatch number: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_loss \u001b[38;5;241m/\u001b[39m batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [121], line 33\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(inp, targ, batch_size, encoder, decoder, loss_function, optimizer)\u001b[0m\n\u001b[0;32m     30\u001b[0m dec_input \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims([\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m batch_size, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(targ\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m---> 33\u001b[0m     prediction, dec_state_h, dec_state_c, \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_state_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_state_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# real value passed to loss_function needs to have shape (batch_size).\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# It is a number representing a word from tokenizer.word_index. Real value = 0\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# means that there was no word\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     batch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_function(targ[:, t], prediction)\n",
      "File \u001b[1;32m~\\Documents\\projects\\my_projects\\python_venv\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\Documents\\projects\\my_projects\\python_venv\\venv\\lib\\site-packages\\keras\\engine\\training.py:561\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39mcopied_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    559\u001b[0m     layout_map_lib\u001b[38;5;241m.\u001b[39m_map_subclass_model_variable(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout_map)\n\u001b[1;32m--> 561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Documents\\projects\\my_projects\\python_venv\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\Documents\\projects\\my_projects\\python_venv\\venv\\lib\\site-packages\\keras\\engine\\base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1131\u001b[0m ):\n\u001b[1;32m-> 1132\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32m~\\Documents\\projects\\my_projects\\python_venv\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [120], line 32\u001b[0m, in \u001b[0;36mDecoder.call\u001b[1;34m(self, x, decoder_state_h, decoder_state_c, encoder_states_h)\u001b[0m\n\u001b[0;32m     29\u001b[0m encoder_states_h \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(encoder_states_h, tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# context_vector.shape = (batch_size, enc_state_size)\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m context_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_state_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_states_h\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# shape of output of embedding layer = (batch_size, 1, embedding_dim)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n",
      "File \u001b[1;32m~\\Documents\\projects\\my_projects\\python_venv\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\Documents\\projects\\my_projects\\python_venv\\venv\\lib\\site-packages\\keras\\engine\\base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1131\u001b[0m ):\n\u001b[1;32m-> 1132\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32m~\\Documents\\projects\\my_projects\\python_venv\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [117], line 27\u001b[0m, in \u001b[0;36mBahdau_attention.call\u001b[1;34m(self, decoder_state_h, encoder_states_h)\u001b[0m\n\u001b[0;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([decoder_state_h[i], encoder_states_h_flattened[i \u001b[38;5;241m+\u001b[39m k]], \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# x.shape = (dec_state_size + enc_state_size)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# x.shape = (1, dec_state_size + enc_state_size)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m e \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mappend(e, tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(x)))\n",
      "File \u001b[1;32m~\\Documents\\projects\\my_projects\\python_venv\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\Documents\\projects\\my_projects\\python_venv\\venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\Documents\\projects\\my_projects\\python_venv\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:442\u001b[0m, in \u001b[0;36mexpand_dims_v2\u001b[1;34m(input, axis, name)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand_dims\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    376\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexpand_dims_v2\u001b[39m(\u001b[38;5;28minput\u001b[39m, axis, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    378\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns a tensor with a length 1 axis inserted at index `axis`.\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m  Given a tensor `input`, this operation inserts a dimension of length 1 at the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;124;03m    InvalidArgumentError: If `axis` is out of range `[-(D+1), D]`.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\projects\\my_projects\\python_venv\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:2815\u001b[0m, in \u001b[0;36mexpand_dims\u001b[1;34m(input, axis, name)\u001b[0m\n\u001b[0;32m   2813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   2814\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2815\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2816\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExpandDims\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   2818\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch_number in range(len(inp) // batch_size):\n",
    "        inp_batch = inp[batch_number * batch_size : (batch_number + 1) * batch_size, :]\n",
    "        targ_batch = targ[batch_number * batch_size : (batch_number + 1) * batch_size, :]\n",
    "        \n",
    "        batch_loss = train_step(inp = inp_batch, \n",
    "                               targ = targ_batch,\n",
    "                               batch_size = batch_size,\n",
    "                               encoder = encoder, \n",
    "                               decoder = decoder, \n",
    "                               loss_function = loss_function, \n",
    "                               optimizer = optimizer\n",
    "                              )\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        print(f'Batch number: {batch_number}, Loss: {batch_loss / batch_size}, Time per batch: {(time.time() - stime) / (batch_number + 1)}')\n",
    "        \n",
    "    print(f'\\nEpoch: {epoch}, Loss: {total_loss / ((batch_number + 1) * batch_size)}, Time per epoch: {(time.time() - stime) / (epoch + 1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01287fe-3046-471e-8839-c24375ec2207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
